{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDmHlX1jdDUl",
    "outputId": "f237c3c6-b8d4-4c63-e6cd-ce6e3ed0de56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.10.1 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers\n",
    "! pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQVi-xChdGU5",
    "outputId": "797a6a25-ec7b-4598-ab71-f2aa11b3ffe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tv0u8hytcBYL"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainerCallback, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "\n",
    "def get_ru_model(checkpoint = \"sberbank-ai/rugpt3small_based_on_gpt2\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def print_generations(prefix = \". \", n_samples=4, max_len=70, one_line=True):\n",
    "    global model, tokenizer\n",
    "    input_ids = tokenizer.encode(prefix, return_tensors='pt').to(DEVICE)\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=max_len, \n",
    "        top_k=50,\n",
    "        top_p=0.90, \n",
    "        num_return_sequences=n_samples,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    for row in sample_outputs:\n",
    "        text = tokenizer.decode(row)\n",
    "        if one_line:\n",
    "            text = text.split(\"\\n\")[0]\n",
    "        print(text)\n",
    "\n",
    "\n",
    "class PringSamplingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        print(\"=====\", \"progress:\", state.epoch, \"=========\")\n",
    "        print_generations()\n",
    "        print(\"==========================\")\n",
    "\n",
    "\n",
    "def freeze_layers(model, n_layers=8):\n",
    "    assert (n_layers >=0) and (n_layers <= 12)\n",
    "    to_freeze = (\n",
    "        model.transformer.wte,\n",
    "        model.transformer.wpe,\n",
    "        model.transformer.drop,\n",
    "        model.transformer.h[:n_layers]\n",
    "    )\n",
    "    for paramgroup in to_freeze:\n",
    "        for param in paramgroup.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def finetune(model, tokenizer, path, n_epochs=\"auto\", adjust_to_size=5e6):\n",
    "    if n_epochs == \"auto\":\n",
    "        filesize = len(open(path).read())\n",
    "        num_train_epochs = adjust_to_size / filesize\n",
    "        print(\"SETTING N_EPOCHS TO \", num_train_epochs)\n",
    "    else:\n",
    "        num_train_epochs = n_epochs\n",
    "    \n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = path,\n",
    "        block_size = 256,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm = False,\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        overwrite_output_dir=False,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        callbacks=[PringSamplingCallback],\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "COLAB_DATA_PATH = \"/content/gdrive/MyDrive/datasets/\"\n",
    "OLDRU_PATH = join(COLAB_DATA_PATH, \"oldrussian.txt\")\n",
    "DOST_PATH = join(COLAB_DATA_PATH, \"dost.txt\")\n",
    "TOLST_PATH = join(COLAB_DATA_PATH, \"tolst.txt\")\n",
    "LENIN_PATH = join(COLAB_DATA_PATH, \"lenin.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxxwY_NxlZD2"
   },
   "source": [
    "# Тексты на древнерусском языке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "df3e9558a2c442ec99d352fc7a94e655",
      "b842b1ad95f8445e9e7bc12664e2437d",
      "283c2885475e42aaa2a91d6d6774698e",
      "eb2196d7d3d34aafa010cc89ac28d67c",
      "77072bba96f245b49e7b6f2359dfe18b",
      "d13ee81975ed4923a5f6afdb104dfa51",
      "16fc3fcd6e7a45008c032a6714bb8fea",
      "82862eaf05a0463e85d57748672b4137",
      "8da6771f27484ef99f721e8f2516a1c0",
      "1b50fb01b9504b3fa62c165ea09efb8d",
      "f8ad791396824a99a5cb32ee4ed2abe0",
      "8d4b2911c90846a295fe9beda5e5b131",
      "f6e3fab9b28e4035a87728317b4f9745",
      "3a76393c12444a40a15e0e92424318d6",
      "f0ab9bc2bd0a4106b009a7843d416c6d",
      "1d2164e5963741a19a75a17fc4cf189a",
      "9b77909843924d7c8bfb7193aaa02e35",
      "f37cb7a17ed14e5db0894004de143887",
      "c1e6a8d6b6ed4f80b0fe6bf8f1fc8ba6",
      "9dc44856f3f6476082f165be4d5633e4",
      "1b4ed3ca990d486fb294ab96854492ea",
      "8df61434b9ee4ce28bdea6506dfc7577",
      "dd1e731d438c421cb55140abe5653a38",
      "6d2fe73c34b848f1a92faaf491a4dbf0",
      "63d424f5ad7b4c7bbd6159ab1bd86a64",
      "20c8a59195d847bc858d3dfbb3098975",
      "83b8479702394ba0ae62d762cf5d3879",
      "87b10810f84a44c9952f573af6ed169e",
      "1a12a378d8584f0eaa8871094e393e02",
      "3b862b1b4070492a915298d1820fa477",
      "8212133bd0b148d39c3a99772308c52a",
      "df0c5d4f42484bc9bd45e9f0f7e25f06",
      "35b7cfeaeb4440b195ac62660daee437",
      "2165a701e63f46d79d95a2f23370288f",
      "3964cbce98614d28a753b3ea6d75ae19",
      "30f06e4dd9534bd18ec055ad0f8b13ed",
      "708fd078cdd3481e984567d4b3184fb2",
      "4daf0a2782194095870c2f8e2b58aa4e",
      "f285b73d6b6043f6b9da603623864ad7",
      "0d844a15082c45c6aaf3b937a058657d",
      "f918b06459eb41f198fa3d25acb7e432",
      "300bb98f8b7a413ea48c59ecb8fcae15",
      "1d7e6d4452c64f44971cfc90ce746316",
      "4279e77945f142e7be40a4fcdf4df5dc"
     ]
    },
    "id": "rMpOOGHct24Q",
    "outputId": "cba82d82-5259-4bc8-9d07-2a2486565d8a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3e9558a2c442ec99d352fc7a94e655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4b2911c90846a295fe9beda5e5b131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1e731d438c421cb55140abe5653a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2165a701e63f46d79d95a2f23370288f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_ru_model()\n",
    "freeze_layers(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z1Xr2W8UUUEk",
    "outputId": "460b9a4a-9ca5-42da-eb57-5e8ed77686f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING N_EPOCHS TO  0.587214714943076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11776\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 865\n",
      "  Number of trainable parameters = 56704512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='865' max='865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [865/865 06:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.948700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.033967391304347824 =========\n",
      ".  Он же, видя что питием бо и от чадъя и от духовъ приимъ. о семъже поели со мною, и крѣпие дѣти въ церковь, а и прииде, и бысть съ пнемъ сь в церковь, и въ ней\n",
      ".  Вѣрное же преставление, яко всѣм бо, якоже вѣдущие., възможенъ житию, тѣла и мысли, бысть, бѣяшшу, вѣтмо мѣсяца, и паки въздержанишися.\n",
      ".  Когда же досточны и отчисто их сѣдили, и оных же съсоръзницъ наѣх и снѣкъ при нихъ, и отъ нихъ же съградоша съ нимиъ, и от нихъ же въ тъмъ же тъмъ же в\n",
      ".  Он же от лица свѣта сьде на лѣта, и мѣськи постави, и от лѣта, и от тѣхъ, но и от нихъ. и яко въ ту ночь от нихъ къ царю и от царя к нему, и на то время къ его. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.06793478260869565 =========\n",
      ".  Въдѣть же несть ея въдѣхь, а сѣя, бо по тѣло, въздохъ. и ея же по прелъстве дыххъ, вниде в раку, и ея сребѣ дыхх, и дыххъ\n",
      ".  И взалъкося, и взалъкъ, и тако, взалъкъ, и тако и заишашаша, и приѣхши, и и приѣхша, и огоша, и приѣхша, и приехша, и приехше,\n",
      ". \n",
      ".  Вългаеже мѣны быша, и сшедше во всѣ монастыри, а инаше в него вздѣлишася. и се, нѣчюже, мнѣ и быша по монастырскому житию и по всѣмъъ псковскимъ м\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.10190217391304347 =========\n",
      ". \n",
      ".  И вот, идѣже, измѣнився, но и вмѣнився, тако рече имъ: \"кто не имеетъ того, не вѣстникъ, но иночестный?  вы льшишъся лишися лишишася, и с нами, такоже\n",
      ".  Егда же он во вси, и не дастъ ему прилога. оть бяше сътворишися, и отьграх ся, и преведе от него и мя, и рече къ немцу, яко: «отъкы». онъ же убо не дасть\n",
      ".  За симъгласите мьстеньствующу: \"заблудитеся и погубите! не по нь си бѣ, но нынѣ!\" и паки убо възложишися на тя. и оттуду не престаниша. и к тому приидоша,\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.1358695652173913 =========\n",
      ".  Пребнѣ бѣти по-прежнему, а наслѣдше въседѣй, аз же, въсиде въ единѣ, глаголюще с вами: «вѣдаете ли еси, да не буду се».  «кто се будетъ и речем, се и будетъ и\n",
      ".  \"аще бо нѣкааши\", то азъ же, прияха къ игуму и вопроша: «о, игуменъ, что хощеть?\" — игуменъ рече: «дани, иже у насъ есть», и глагола: «и не боши ми, о\n",
      ".  И то бысть жѣ в мъстецѣ, а в том мѣсто — не бысть. да и ни в чем бысть: \"не вѣду мѣсто на небо\". убо бо есть в темнити многа и мѣсто на землю, яко в той земле сечеши,\n",
      ".  И се нарекаше, яко: \"о, что възможеши, о, что ми, о, что намъ не нападаше?\" и бысть, и възможеши, о, что ми събѣшеть? како же се?\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.16983695652173914 =========\n",
      ".  \"всем же, убо дьрва, да не съзрѣшати иныя: ни убо ся, ни убо отъ меня; и нынѣ едина многа дѣла и многа чюдности и възращающа. сему жити, яко не ток\n",
      ".  и рече: \"не тужи, си си, не хужи, а яко хощеши и присно тебе. иже в лѣтѣхъ твоем, не молилъ Бога, а остави мя богъ, иже от рускаа, еже въси от начала и до низ\n",
      ".  А иже въпроси, постави, уразуметь си, тако да просветлешися, присно разумѣ и о блаженный духъ, еже сътворяетъ единаго вѣрца и творятъ мнѣти о всѣхъ оныхъ; не убо, яко бы\n",
      ".  и мнози бѣ воину въ тѣсѣхъ. аще же не учиняше всему, аще же онущешя, то отже се творящю вѣрѣ, такожде и всѣхъ: онущашуся, и егоже не могуша\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.20380434782608695 =========\n",
      ".  Яда же у негоъ не было, и вои на конныхъ и на зъ коней, не побѣжавшися от них, а иные в ту же вечерину по вся дни в келии своея молишящеся имъ. \n",
      ".  Вълѣзая по седому древнию мосту, узреть бо в землю остави и от златотка. он же с любовию своею святую икону святителя, яко святую его икону в лѣтъ своего мѣста, и оставиша икону его, иже в руцѣ\n",
      ".  Онъ же, видя, яко не по себе, и не можѣся отъстояти от мене, то и не взимайся на дружины своих. и не обрѣтоша их и сею, в поле и в ратуши с сѣбѣхомъ и с дружины.  И\n",
      ".  Аже о мнозе, в нейже нисми. оже мноза и яко нѣкый мздоимѣстникомъ своимъ былъ, но иже отлѣжникъ мноза и едина ему има, да яко бывъ борисѣи и рече: “\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.23777173913043478 =========\n",
      ".  И тако на мясе и на зѣло, и тако на мясе и на зѣло и на ептарехъ и на въпихиихиихиша. и тако на мясе и на ептареху и на ептарех и на бяше,\n",
      ".  Одурети бяш, не доставиши к отцу своих, в то время же, и на нь, отцем своимъ, и самъ злая, отъ брата своего, яко рать, и по вся дни не оти. самъ же. в том же лѣто 6345.\n",
      ".  И рече ему: \"сеи бо еси: аще хощеши на нѣтмо, да не отринут в землю си иже есть въсхотѣнии, то с тобою будеть земли его, аще и нынѣ быхъ былъ, отрину бых, да и\n",
      ".  Они же, понеже, христовы дары разлучением пречистых и преславным преставлением во мнѣх причастию, и вѣры, и вѣры, и грѣх, и мьстицѣм. иже възбрало вѣры свое, и по\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.2717391304347826 =========\n",
      ".  \"въста бо есть до пѣнии поѣдъ\". а въста, яко не по нозѣ пѣдѣти, и рече: «не хощу отиже въздѣ». тѣло то, слово рече, да не вѣдати, ни поть, ни\n",
      ". \n",
      ".  И точию же убо зрѣти и приидоша к свѣту богу, и положити в церковь.  и оттоле, а в то же время съкрови его свѣтися от всѣхъ оныхъ, и глаголаше: «иисусе нашь,\n",
      ". \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.30570652173913043 =========\n",
      ".  И паче убо изищу, и съзноваю, — глаголють. \n",
      ".  А ты у тя в китцѣ былъ. и все. а то у насъ толькы ничьныя оттуды и делися, а мы у тебѣ такъ и званъ былъ, не то не можеши, чего можеши сотворити. и не мы\n",
      ".  Святый же апостолъ же от злата и от крестиславовъ уде и́ отъ злата в землю и от святаго свѣта и от сребро и славно, а от сребреника и от отроча, и от отроча, и отроча отрока и отрока\n",
      ".  \"уготовила нѣкого князя убо и възвести въ храмъ с нимъ и сядуща с нимъ с братомь\", понеже и въспомянуся и моляся ему: \"кы буди ти с нимъ? и поидѣмъ кы ему, но ему\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.33967391304347827 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Configuration saved in ./checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  Но, когда же от того бываша имаши божиимъ поити въ обитель и съдѣти бысть, и паки бысть тамъ.и по подобании въсхотѣ нирсиши имъ. и поставиша к тѣхъ же во обители и, оттом\n",
      ".  И повелѣше даниллу на брань даниллу, и с нимъ со служби срѣтоша о себѣ, яко бо в домѣ своемъ онъ, нъ отлѣжи же ему сътворити. но змилостивь к великому князю и бояре своему да\n",
      ".  Самъ же святый, по евангельскому образу, приступивъ с пастырскимъ руководствомъ и от пастырскихъ обязанностейъ к тому, како по многыя добродетели бысть ему приити к единому от них образу, не токмо исповеди, но и прилучи к нему, и так\n",
      ".  Вѣсть и приять, и не имѣти на него греха, яко не не имѣти на его греховникъ, ни мѣсяць от бога възмѣшение, ни мѣсяць мѣсяць чрѣзьнѣцю. онъ не имѣть, ни\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.3736413043478261 =========\n",
      ".  Господь бо с намиъ начатъ, и не внидо ти. и тако тѣ же преставится с нами. и паки преставится от мертвых. и възможе богъ нась богъ на всяку болезнь, и възнесеться съ нами. не възможеши бо\n",
      ".  Но аще есть въ церкви святаго отца нашего кирила и во святых отець сих, преподобне, и, яко богъ, аки зверь и человѣкъ, и въ своих святыхъ книгахъ писано бысть. аще бы кто и на томъ соборе во церкви бысть, ни во святыхъ отца\n",
      ".  а аки олъ, яко и ти есть и во граде. аще бо не въсѣлѣ, не възлюбяще. аще бо, но вѣдай, но въставься. аще бо есть, но вѣре, не бываеши. аще бо, то\n",
      ".  В день святой богородица не остави, и в молитвах оных не впадоши.  отнищи же тогда своя рука от друзии его, и в снѣ уподобихся от нихъ, и не преклонися ницѣ, но предаше ихъ господеви и на молитву\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.4076086956521739 =========\n",
      ".  \"не можетъ бо ся зрѣти, якоъ такоже и в домѣ своемъ пребывати. и от того, что по всей\n",
      ".  И тое и другое не повѣсть инея, иже и въ лѣто прииде вси, да не сътвори. и, слышав, князь великий, егоже князь тысяцкий, со всѣми своими рать, и мнози с нимиъ, и рѣхомъ, и\n",
      ".  Тогда же, по сему повѣдаша сице: и дасть имъ бѣ дѣти, иже уразумѣвше, отиде к нимъ, и възвратишася къ нимъ. имѣя бо ихъ нѣколико дѣти и вънидуше\n",
      ".  Да и не только, а иныя есть в чюдесе. а ты, яко быти, не уведе есмо, но с прочими святыми по вся дни по сребрянымъ имѣниа. а о хвале, есмо, — какоже умыслиши, или како\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.44157608695652173 =========\n",
      ".  И повелѣша убо царю царю: \"о, царь, крѣпкими вѣры, яко и многа лета вѣтрамъ, и яко мнѣ азъ мняше имаши от вас и до сихъ, но от сихъ, не извольте от вас, ни от иных\n",
      ".  О немъже имѣти крѣпкого вѣстника и побѣдоша его от него. и не дасть ему корина или мниху. тот же корин, вѣрже ему в себѣ, нача ему со всяцѣй руской земли и и с руси со\n",
      ".  Пьех и рѣховъ, нѣкы же зѣяниих, бѣхъ неистовъ, и отнидохъ, яко нѣколико вси зѣи. да будеть имамъ боиваа и воставь. воскресь и приди,\n",
      ".  Приложи же к руцѣ и не возможе ли не дати мнѣ мѣда? и ничиюже мнѣ есмы не имамъ, ни мѣста не быти и не можеши дати мѣста. и не можеши ли хощетъ? и вѣ\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.47554347826086957 =========\n",
      ".  А в мѣсто то есть на руцѣ сѣдлех и на горах, и аки сѣдлех сѣдлехъ. но александръ, великий и любимый, от него коида же приатти? александръ же самъ рече: «я\n",
      ".  Тако же и на манастырь, и на павловъ, и на отьцѣ, и на змия, и на дьруг, и на христа ради, и на пресвятую троицу. по сих же внидохъ, яко и нѣкто бысть, глаг\n",
      ".  Приидоша же князи, зръдъ и прасть, прииде в раи на себѣ, и рекоша ему: «брате и нь, и тъ не ослугайте евы. и азъ ослушаюся и не ослушаюся. и не ослу\n",
      ".  И крѣпко рече мне: \"не можеши, отче\". и пристави ми руку, а онъ рекоти тако и не прииму, не имати. аз же рече ему: \"не можеши, отче, тое възъевати кѣмо, да\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5095108695652174 =========\n",
      ". \n",
      ".  Оттого же сего, когда и я вѣду, но и понеже нѣчто от ина дѣлеся с нимъ с нимь дѣлующем и многа има, аще бѣло, но егда бо бывъ на ипѣ и тъчашеся,\n",
      ".  Сийъ, глаголетъ, что, и мы, имаи, его не хотящю.\n",
      ".  \"и паки сподобся, и вѣдаше, яко мѣсто есть — святый камени. не постой ми во всем, ни в чемже вѣрою и нечестиаго, ни в чемже, но и вѣрова и разумѣва\". и тогда святый апостолъ рече\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5434782608695652 =========\n",
      ".  Александръ же не инако глаголеть: \"молися о ней, а не молить о ней\". и рече влѣсникъ: \"но се убо приидетъ съ нами, да поиде, — и прииде, и от насъ от насъ отвѣщеть\".\n",
      ".  а с нѣмцем и съ слезами, и со слезами, и со слезами, и со слезами, и с молитвами. а с нѣмцем и слезами, и со слезами, и с слезами, и со слезами, и с слезами, и со слезами, и со слезами, и со слезами, и со слезами,\n",
      ". \n",
      ".  И те же, и те же, да и по два-три в год с половецкой войны воеводы его. и до сего времени воеводы наших да на три стороны в осаду его бяше, да по два-три раза по двадцать. и от сего времени воева быш я, яко нѣкох\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5774456521739131 =========\n",
      ".  А также до вечера, понеже възвещаю, и в немь будуть, аще ли въмьчь, тогда имѣемъ внидоша въ кыевъ. от нихъ же и доиде в цесарьскиа, на то же время убо не преже вси\n",
      ".  аще есть в чем не тѣмо, аще же есть, но есть в чем, и се сътворити възбратиша, якоже и до сихъ. точию и въ всѣхъ еси. аще же есть, или не въста еси; вѣдаеши ли\n",
      ".  Яже есть по земли, и в ней есть мѣсто, и мѣсто, иже елли его отьця, и съмь, якоже елли, в нейже, бѣса, и даждь и до гроба вниди. и се есть мѣсто, и в\n",
      ".  И на свѣтѣ вѣмъ вѣрѣхъ имѣя, тако иже, иже въ мѣсту христосъ, яко не вървутъ тѣло, не възвысятъ от земли, на землю, не възъгохомъ, не\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5876358695652174 =========\n",
      ".  Да, рече о них, яко многа к нимъ приити. се бо имѣния.  и приидоша ко граду. и пришедша ко граду крѣпцы, и взиде на мѣсто имаша, и приидоша к граду; и поставиша городъ и\n",
      ".  И в томъ часѣ не быша от мала, ни великаго чюдеса. и на томъ часѣ не бысть. и на томъ часѣ не бысть. \n",
      ". \n",
      ".  и тако възрасту въ блаженной,\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to .\n",
      "Configuration saved in ./config.json\n",
      "Configuration saved in ./generation_config.json\n",
      "Model weights saved in ./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "finetune(model, tokenizer, OLDRU_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01pyN3LacPPE",
    "outputId": "8b1fb460-cd82-47bd-aebe-7c69c0d130c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самое главное это вѣрие,\n",
      "самое главное это воя, а иежеи внидуще в немъ.\n",
      "самое главное это слово». иже от того времени ина единаго, якоже от бога о сем есть, иже и сам бог да не возненавидят, да и не возненавидят.\n",
      "самое главное это\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"самое главное это\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Dkr1shpjxob",
    "outputId": "eb687721-d8bb-4cfb-9762-48c382c872b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "зачем и кто и как? не измѣютъ, да и какъ будетъ, не знают, а кто и какъ будетъ, а кто и какъ будетъ, а кто и какъ будутъ, то не ведаютъ.\n",
      "зачемъ сѣче его на пути, сыйже въ агианѣ, и иже поставиши на пути его». и, не въздержавши, паки внидоша и сѣче сѣче на пути его, и иже постави его на пути его, и паки положити его на пути его и спяти ему от него, аки въсконь его питися. и тако прииде вѣтренъ, и дошед до земли, овии же, паче всехъ своих, и рече: «время, отьцю. тако и вы, отче, на земле живя, каяйтесь и молим тя. да будеши ны». и бывъши сѣчу, приидох до земли, и рече, — глаголаше господь: «видѣх васъ, въздаша ми тя». и пре\n",
      "зачемъ, но, аще убоится имѣния, имѣющу бога вѣра его. вѣрь ми велика, в нейже азъ от него, яко ничию же и мняше, ни сѣдою, ни сѣднаго, яко богъ, сътворять. яко же не от бога, а от бога от бога и отъ\n",
      "зачем ми, яко же быти, или, яко не хотѣл быти, тако и быти ми не вѣмь, и тѣмь жити на мѣстехъ.\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"зачем\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvRZc_FvWu1I",
    "outputId": "4d413fc6-18a8-4e63-fe6c-3470ef0f647d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "когда от нее, оже глаголаше, оже глаголѣти: «съдѣте на небо и повѣдайте, яко дѣло и воля ваша суть, егоже бо есть богъ. и възбранно и христолюбезное его есть иже есть есмы, оже не вѣру от насъ творяху, ни единому другому же с вами не обрѣтуя. да аще бо вамъ чего имать на небо и повѣдати, не хто ли ми вѣдаю вамь и есмы, аще бо вамъ о томъ есмы, и не хто ли ми вѣжду?».\n",
      "когда онъ же сядя и рече к ней: «видѣ, брате, что нѣсть поѣда в той градѣ и нѣ что есть въ немъ? или нѣкогда? и есть ли у тебя вѣсты, а у насъ съ тобою вѣстники и дѣла дѣяти и дѣлати, и дѣлати есть приати въ рускыихъ городѣхъ? и не смѣемъ ити. и аще и нѣсмь кто достоитъ на немъ, то, брате, от насъ мняшть с собою крѣпкие дѣли». она же оттудяше и повѣда ему, како есть и каковы дѣти, а самъ приати в русь, а самъ въ русь. и оттуду же бѣше у нее в русь. она же убо,\n",
      "когда у нас много народу, от вас много и ты, не жалей о нем, а ты молиши бога и нас, чтобы тебя и нас на мя не обижали! а ты, брат, ни у кого не бѣдай, а у нас мало, а у нас все есть, и ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты, а ты\n",
      "когда, да иже о тех, молящуся имъ? аще ли има от нихъ, молящу им, и имъ не повѣдаю. не быста от нихъ, но вѣдомъ, да не вѣдаю има». и\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"когда\", max_len=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAo4MHUdlh_y"
   },
   "source": [
    "# Собрание сочинений Достоевского"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HBtdtv4klkoF",
    "outputId": "a1138a1f-5739-42a5-c00a-31a1ecd8d494"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING N_EPOCHS TO  0.34442475176791504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file /content/gdrive/MyDrive/datasets/cached_lm_GPT2TokenizerFast_256_dost.txt [took 0.940 s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14590\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 629\n",
      "  Number of trainable parameters = 28353024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='629' max='629' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [629/629 03:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.560700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.027412280701754384 =========\n",
      ".  Он, по крайней мере, понял, что с ним делают. – Он уже хотел было сказать, что с этой женщины – хоть это и было для него все же… то есть, видимо, потому что она была почти что и так не такой, как была. \n",
      ".  В его глазах читалось много, в том числе и сожаление, о том, что даже несмотря на это, не мог он и мечтать быть рядом с ним.  Они оба были в этом единении, но как часто он в последний раз оборачивался!  Даже в глазах его и его сестры не было сожаления или сожалений. \n",
      ".  Когда же дотащила его до кровати, он с негодованием, и даже со страхом отшатнулся от нее, посмотрел на нее и не нашел ответа.  Что касается ее, то теперь она поняла. \n",
      ".  Он сказал, что он не сможет жить без нее.  Мне он обещал передать это в письменном виде, но он не верит.  Что за письмо от тебя он получил?  Неужели ты, наконец, веришь?  И если веришь в то, что он может умереть, так не умирай, а прими его как дар. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.05482456140350877 =========\n",
      ".  В этот период еще в XVIII веке была в моде маскарад.  Но ее заменили шляпа и платье с пышной, пышной головой и длинными ногами.  С помощью маскарадных масок были преобразованы даже самые знаменитые в истории Франции маскарады. \n",
      ".  И если бы вы с таким удовольствием и как следует и, особенно, с удовольствием, принялись за свое дело, то вполне могли бы и потерпеть в этом отношении неудачу. \n",
      ". \n",
      ".  В таком случае в суд приходит еще несколько тысяч граждан России, но они не могут решить, кто задает вопрос: так ли все это было в действительности, и как они поступили бы в случае какого-нибудь скандала.  Вот и получается, что по сути дела, все, что здесь описано, уже давно придумано и опубликовано: \"С\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.08223684210526316 =========\n",
      ".  Он говорил, что это для него и есть самое лучшее: все его любят, а сам он очень счастлив и все ему прощают. — Вы мне о всех обо всем говорили? — спросил я в первый раз. \n",
      ".  Все три недели я так же, как и прежде, ходила туда.  Каждый день в школе я была так же как и всегда, с каким-то лихорадочным любопытством.  И если меня особенно интересовал учитель, то, может быть, еще и потому, что я уже к тому привыкла.  Я так же часто, как и\n",
      ".  При этом Вы можете написать мне, о каком письме вы просили бы написать.  Для писем о том, что Вы хотите от меня, пишите мне.  Я сама иногда не умею писать.  В письмах, написанных в эту пору, не осталось и следа моего письма.  Я теперь пишу письмо исключительно для своего наслаждения.  Я не\n",
      ".  Ну и вот... \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.10964912280701754 =========\n",
      ".  Если вдруг она спросит, если вдруг будет в чем упрекнуть, то это в самую минуту.  Так что вы даже не можете себе представить, как она могла прийти сюда и с вами разговаривать, даже не видя меня.  Даже при первом же упоминании имени, вы бы заметили, что она начала говорить.  Это она сама и\n",
      ".  Все вы знаете об этой трагедии, я также был свидетелем. Но все равно я чувствовал, что вы говорите обо мне. Все это очень похоже на правду. Вы сказали, что все вы видели меня, но я не помню, кто, кажется, видел вас. Вы говорили, что видели меня. Если же вы не знали, зачем я приходил\n",
      ".  В этом смысле он был весьма похож на всех, кого я когда-нибудь знал и о которых слышал, но, в отличие от многих, он был более-менее порядочным и не совсем умелым человеком.  Мне было ясно только одно: он не знал о том, что он нанял меня и что это на него с таким настро\n",
      ".  И только на то время, когда он в третий раз, как он уже не мог поверить, но все еще не веря, как он тогда не был в силах поверить и не мог, и не мог еще поверить, в один раз, и вдруг, когда это произошло, как и когда не мог бы поверить, не в первый, и не\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.13706140350877194 =========\n",
      ".  Из всего сказанного мною следует, что в таком случае я буду говорить не об одном лице и не о роде в целом, а как о совокупности лиц, которым должно быть поручено мое поручение о составлении и отправлении в тюрьму человека, назначенного в моем ведомстве, по смерти которого была произведена ужасная казнь в отношении того, который, несомненно,\n",
      ".  Во всяком случае, я помню, как именно он мне вручил ему это письмо.  Он был так взволнован, что уже и не помню точно, кто он, – я помню, он, не помню, что его лицо, его лицо было удивительно неподвижное и какое-то грустное, – но точно какое-то лицо\n",
      ".  Я и сам могу в этих делах разобраться.  Пришла в такое расстройство; как это там я могу, при свете дня и при свете дня, по собственной милости, при одном только моем сердце?  Вот ты, я знаю, что тебя не обманешь! – перебил его Илюша. \n",
      ".  Все это хорошо, но почему-то я не знаю, как это сказать; но то, что я сказал, было что-то вроде \"милость моя, что это такое, если я так тебя хочу\". \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.16447368421052633 =========\n",
      ".  Я думаю, все эти люди не на одно дело учились, на одни и те же задачи.  По крайней мере, в моей семье.  И, несмотря на то, что многие из нас не могут жить, как я, они с этим, я думаю, живут.  Ведь в наше время, не знаю почему, все так по\n",
      ".  В этой же ситуации, но при еще большей силе и при более резком движении, но уже и более быстром и более медленном продвижении, но в более резком падении, так что в этих случаях она, в свою очередь, сама в свою очередь, не удержится и остановится, и тогда уже, даже и не разобравшись\n",
      ".  Он был в это время один и даже без отца.  Но, во-первых, он не только был одним из них, но и даже в этом случае, был еще одним.  Это был тот еще, и если бы он мог иметь и двух сынов, то с одним из них он бы никогда не решился уже идти на преступление,\n",
      ".  А если вы пришли из России в Америку и начали смотреть на наши денежки и на нашу землю, то это совсем другой народ, совершенно иной язык и совершенно иной образ жизни! \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.19188596491228072 =========\n",
      ".  И если в том, что я не могу назвать еще одним словом, может показаться странным и неправильным, то в этом отношении я, как уже говорилось, еще более чем могу быть довольно доволен.– Сударь, может быть, это какое-то преувеличение, и, может быть, не только преувеличение, но и какое-то\n",
      ".  О!  Теперь я понимаю, зачем мне нужен этот весь этот дом с его надменными и насмешливыми лицами, которые, впрочем, на редкость и часто раздражают других людей, так? \n",
      ".  И, наконец, он был в это в то же время ужасно доволен собой, был как будто в каком-то состоянии. \n",
      ".  Они все, конечно, помнят, какие у нас в городе хорошие дворники, но все же… \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.21929824561403508 =========\n",
      ".  Здесь я только что прочла, что он за птица: \n",
      ".  Как только с нею познакомился, а я не знаю, на что годитесь, так и не решилась ее принять; так и сказала ей: \"Я вас только к одному приглашу, а вы меня не примете\".  Она согласилась; я с ней не соглашалась.  Вот я и говорю: \"Я вас одного из вас пригла\n",
      ".  С этой точки зрения можно предположить, что и для \"великого князя\" было более выгодно как раз то, что ему в этот день предстояло явиться именно на богомолье.  \"На богомолье же, – добавил он, – я был бы рад\". Тут же прибавил:\n",
      ".  Если уж вы любите, то вам пора знать, что для вас в этом доме лучше всего. – Я бы еще в этом доме жил, и это хорошо; – Вот он, мой дом. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.24671052631578946 =========\n",
      ".  И мы, все как один, все из них, и все мы, все вместе… Но ведь так хочется, что уж я, пожалуй, могу с ним спорить… – вскинулся Иван Петрович, не слушая, впрочем, ни одного жеста, в котором и он, и все собравшиеся в зале были убеждены; но они слышали\n",
      ".  У нас есть возможность в вашей игре играть в любом виде. \n",
      ".  Он, как и всякий человек, очень похож на своего предшественника; но я думал, что он уж слишком умник.  Я не мог его понять: то ли он совсем не глуп, то ли он действительно не в своем уме.  Я был убежден, что он сумасшедший. \n",
      ".  Вышлите же Вы мне их на дом.  Я их очень прошу, умоляю, и очень умоляю. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.2741228070175439 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Configuration saved in ./checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  Но, когда-нибудь, в эти дни и минуты я расскажу тебе один пример. Он был очень подробно рассказано и не было каких-то особенных подробностей, никаких пояснений.  В то же самое время он был так же подробно записан и несколько, особенно, когда я в последний раз его читал, во весь роман, \n",
      ".  И, наконец, в этот день у Петра Алексеевича был свой собственный день.  По-настоящему он с утра собирался идти к портному, чтобы в последний раз показать свою рукопись.  Но в этот час он должен был быть у портного.  \"Я не могу, - подумал Петр Алексеевич, - ведь здесь не так, как\n",
      ".  О том, как на этом же самом месте в тот же самый день, ровно с полчаса назад, я и подумал.Попробовал на свою голову: \"Почему?\"  Нет, не потому, что я чего-то не понял, не оттого, что не знал, и не оттого, что не понимал, и не потому, что\n",
      ".  В \"Брюсселя\" же, несмотря на все свойственное его особому пристрастию и даже в его убеждении, на этот счет даже не думали, и, вероятно, сами чувствовали, что, если бы не я, и вообще не были так несчастны, я бы все еще мучился и мучил бы и м\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.30153508771929827 =========\n",
      ".  И мы с ним тоже на минутку, как будто бы в самом деле расстались.– И как-то же мы с ним так расстались? – спросила Соня, глядя на него, с невыразимым ужасом смотрящего в глаза.  Но он все еще ждал, что вот-вот скажет, но не мог начать.– Он меня не любит\n",
      ".  Но не все они были такие и уж, конечно, в сущности и не все так уж и плохо.  Вот, например, к примеру, в \"Крестителе\" есть такой эпизод, который я знаю как раз до того, как в нашем театре появился первый герой.  Он не случайно был убит, он действительно убит, как я\n",
      ".  Это же самый лучший! – сказал Алик. \n",
      ".  В начале своего пути я не мог решить, зачем и почему я приехал сюда в такой дорогой и не такой счастливый день.  Я думал – потому что я люблю быть в курсе и, наконец, мне, возможно, не хочется быть таким, как я. Но в эти минуты мои мечты о моей скорой и неотложной поездке к вам на родину\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.32894736842105265 =========\n",
      ".  В начале тридцатых годов в это же самое время, в сороковых, к великому удовольствию немцев, в Петербурге было уже много и в то же самое время много всего.  К этой теме, в том же году, при жизни моего друга, который был в ссылке, о которых сейчас уже очень и очень хорошо известно, принадлежал он,\n",
      ".  Все-таки надо дать ему время и пройти через это.  А если и пропустим, то зачем нам в это дело вся эта грязь, чтобы потом не идти к нему, чтобы не быть ему не на своих же ногах, а к другому человеку, к чему, к чему идти с такой грязью?  Вот и получается что-то\n",
      ".  В этих словах, с виду как будто наигранная улыбка и все-таки в лице была улыбка, была какая-то странная, удивительная, почти до отвращения простая, в общем-то, и в то же время была какая-то необыкновенная, странная, словно бы какая-то волшебная. \n",
      ".  Да и вообще, я в таком смысле не вожусь, – сказал он. – Вот мне кажется, у вас есть один очень серьезный и очень важный вопрос.\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.34484649122807015 =========\n",
      ".  Мне показалось, что тут какое-то что-то было другое.  И в особенности, что-то не похожее на правду. \n",
      ".  И хотя, по сути своей, она не была похожа ни на какую другую женщину; но что-то во взгляде, взгляде, в выражении глаз, в каждом движении этого лица, в каждом движении его головы и в каждом движении этих движений было что-то странное.  Оно как будто не отличалось от обыкновенных людей ни в чем.\n",
      ".  Огородная часть, как всегда, довольно своеобразна.  В основном это дома крестьян.  Захоронения же можно наблюдать за частными, на случай, если вы проживаете в другой деревне.  Сходные деревянные строения тоже вполне могут быть похожи на дворы крестьян.  В этих местах есть еще и сельские избы,\n",
      ".  По окончании работы необходимо сообщить в комиссию по делам несовершеннолетних, и тогда уже и мы сами будем в полном порядке и совершенно по закону. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to .\n",
      "Configuration saved in ./config.json\n",
      "Configuration saved in ./generation_config.json\n",
      "Model weights saved in ./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_ru_model()\n",
    "freeze_layers(model, 8)\n",
    "finetune(model, tokenizer, DOST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9xBNKA5WlRa",
    "outputId": "25723779-a1ba-4c0f-ea3a-632e7b7f5b37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Самое главное это – чтоб все-таки и в ту же минуту не случилось. «Нет, – подумал Павел Павлович, – он не может быть в эту минуту так бессилен, что не смеет переступить через себя». Он подошел к кровати и стал в изголовье, как бы обдумывая какое-то решение. Он не знал, в какую сторону броситься, чтоб не только убежать, но и спастись. Он уже приготовился бежать, но как будто бы вдруг как бы что-то изменилось, что-то исчезло в нем. Что это было? Почему он вдруг опять ничего не видит? Что он видел?\n",
      ". Самое главное это не в том, что это будет, это не то, что это будет; а именно в том, что я уже говорил, то, чего я не могу сказать. В том, что я еще не знаю, что это будет. Но, может быть, и в том, что я это все понимаю, и если я не говорю этого, это, вероятно, не есть, но то, что я говорю, если я не говорю этого, я все-таки говорю это. Это было бы слишком неубедительно, потому что я и так знаю все, но ведь и так было бы довольно, даже слишком легко и просто; ибо как человек, который много знает, все это поймет, – он не может быть до того, как в этом будет нужда. Ведь если он хочет узнать и потому что так нужно, и потому, что он хочет жить, то пусть он знает, да это ему все равно не надо. Я это еще не видел, и, может быть,\n",
      ". Самое главное это найти и понять это. И не в том, что вы не хотите меня учить, а в том, что я не хочу, чтобы вы и я, наконец, знали и понимали, что все это для меня так, как вы, потому что я для вас все равно останусь ребенком, а не лишь ребенком. Вы не любите меня, если я не буду любить вас и в том же случае, если это вас не обидит, я бы, конечно, не решился быть с вами и остался бы один. Вы не хотите сказать, что я не умею нравиться, что вы хотите, чтоб я не мог любить вас и что я только вас люблю?\n",
      ". Самое главное это, что вы никогда не узнаете ничего, чего бы я не знала: где был я и что было со мной.\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\". Самое главное это\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4FiLPepXfzR",
    "outputId": "8b978b53-7baf-4911-94dc-2f6d44b5bd05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В 2024 году (в связи с «Сонмами») был произведен ремонт дома на Рублёвке, но не в здании, а в частном доме на Старом Арбате. Не знаю почему, да и зачем так делать, у меня в том же доме ремонт был, а я и не помню его, так что это скорее к слову… А до тех пор было дело, и я не знаю как в этот дом попасть.\n",
      "В 2024 году в России по предложению В. И. Ленина в письме к одному из знакомых в Москве «о новых вещах», он был поставлен в один ряд с представителями духовенства и с некоторыми из приближенных, бывших еще в то время членами правительства, с некоторыми из которых он и сам встречался иногда, например, еще в Петербурге. В письмах к В. И.\n",
      "В 2024 году, после смерти графа Разумовского, этот молодой человек в своем кабинете, над письменным столом, продолжал писать. Так как же мог он так много писать, то, конечно, не мог не подумать, что его непременно убьют. Он так и сказал вслух, что он и есть убийца и убийца.\n",
      "В 2024 году этот период становится для крестьянским юношей самым трудным. Он еще не знает о том, что его ожидает, – это не только то, что все дети в эти годы будут умственно развиваться и в самом деле будут очень упорны в овладении крестьянским словом, но и его общее презрение к себе. Уже в этом году в деревнях начинают сознавать, что есть и другие, менее способные дети и что именно это их, крестьянских детей, и делает, когда вырастают до того, что в деревню вгонят. И вот что случилось с ними потом, что в крестьянских семьях начали возникать новые страсти к тому, что говорят: «я так и буду теперь все по-мужицкому, по-братцынски, по-прадедальски», – они теперь, пожалуй, перестанут понимать и презирать их. Приходит в крестьянский дом и спрашивает отца и мать: «Мам, ты меня любишь?» И они тоже\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"В 2024 году\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoZLIoJXxHX",
    "outputId": "7771a4b9-90ac-46d1-89ce-1196fde29ca5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Больше всего на свете я ее боюсь, – отвечала она тихо и нежно. – Я в нее хочу превратиться и стать ее матерью. И я в это верю, потому что… я… Я, наконец, она, я… я… Я…\n",
      "Больше всего на свете я хотела быть его… как будто это так и было. Убить…\n",
      "Больше всего на свете я в восторге! Это было одно из моих самых счастливых воспоминаний! Я была в восторге. Она была счастлива, в восторге и даже, кажется, чуть в слезы не билась, но я чувствовала, что она плачет. Когда она выходила, она уже была в волнении. У меня были дрожки в коленях. Как мне хотелось ее поцеловать, но она сама мне все целовала, а я только в недоумении глядела на нее… Она целовала меня во весь рот, потом вдруг, в порыве благодарности, стала ее целовать, целовать на глазах ее блестели слезы, и так она в упоении целовала меня всю ночь! – И она покраснела.\n",
      "Больше всего на свете я ненавидел эти деньги. В тот же вечер я принес эту вещь. Я положил ее на стол, положил, открыл… я взял ее… и, клянусь, мне показалось… показалось, что я… все… все… как… это… я… в… в… это… я… в… вот… – бормотал он, все, что знал про это.\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"Больше всего на свете я\", max_len=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpGyzMHDllUL"
   },
   "source": [
    "Собрание сочинений Толстого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "szsa0CjmlnMP",
    "outputId": "2a308226-1306-474c-e8fa-236d719f0fd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING N_EPOCHS TO  0.8218274747444199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file /content/gdrive/MyDrive/datasets/cached_lm_GPT2TokenizerFast_256_tolst.txt [took 0.704 s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6018\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 619\n",
      "  Number of trainable parameters = 28353024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='619' max='619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [619/619 03:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.367000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.06640106241699867 =========\n",
      ".  Он, по своему внутреннему убеждению, и не хотел быть на стороне.  Он просто не мог допустить того, что, по его мнению, он все же имел то, что имел. \n",
      ".  В зависимости от характера проблемы мы можем получить очень интересные и оригинальные предложения, которые заставят вас задуматься о развитии, работе, карьерной и личной жизни. \n",
      ".  Когда она была у нее, он, как он знал, не мог заставить ее взять ее.  Он чувствовал, что ее сердце разрывается от любви.  Что она знала, что ей хочется его. \n",
      ".  Он сказал, что он не сможет жить без нее.  Мне он не нужен. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.13280212483399734 =========\n",
      ".  В Москве в декабре этого года не было дождя.  Мы уже привыкли к этому, как к новому, что по утрам, приехав на базар, мы слышим: \"Поезжайте на базар, а завтра – обратно в Москву!\"  А если дождь все-таки начался, то мы не слышим, как ехали назад. \n",
      ".  И если бы вы с ними, и они с вами, вы были бы так счастливы, что в этой жизни так просто не поймете своих страданий. \n",
      ". \n",
      ".  В нем не только в самом начале, но и на протяжении всего этого времени был и его руководитель — генерал-майор граф Алексей Дмитриевич Рогов.  В этом издании есть также его автографы, но только, по словам того же Кити, при нем. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.199203187250996 =========\n",
      ".  Он говорил, что это для того, чтобы показать ему, как его любит княгиня Ольга, и чтобы он не забывал ее и в доме ее.  И он не забывал, как хорошо чувствовал ее. \n",
      ".  После того, как мы все сели за стол и я, как мне показалось, чувствовал себя в новой жизни так же хорошо, как мы с Тосиным, я чувствовал себя по-новому: чувствовал себя так, как чувствовал себя в свое время и я. А когда я ехал к Тосиным, я так же чувствовал себя так же\n",
      ".  При этом многие знакомые уже видели, как я езжу на своем скутере. – говорит Наталья, — когда-то в нашем дворе была очень простая деревянная скамейка для собак, и собаки его боялись.  А сейчас, если собака будет одна, то, значит, будет много собак.  Мы всегда ждем, когда придут друзья. \n",
      ".  После чего все были отправлены в штаб-квартиру. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.2656042496679947 =========\n",
      ".  Пребывая в нем, человек невольно соприкасается с жизнью и чувствами других людей, с своими поступками, чувствами, понятиями, понятиями, понятиями, идеями и убеждениями.  Но в человеке, в глубине души стремившемся жить своим умом, в нем нет никаких этих духовных ориентиров.  Напротив, именно в человеке,\n",
      ".  Все вы знаете, что это такое, вы – моя подруга, которую я хочу видеть, но вы – не жена.  Как вы могли догадаться, что я буду жить с одной женщиной, которая, с вашего слова, мне изменила, и вы можете считать меня сумасшедшим.  Вы не понимаете этого, но не понимаю.  И вот\n",
      ".  В этом смысле то, что я такою дорогой шла, я была бы счастлива и не была бы дурой, которая, взявшись за шпагу, шпажет, так и не сделала того, что я могла, я была бы счастливой.– Так ли это? – сказал он, отвечая, – но это неправда.  Я не\n",
      ".  И только на то время, когда он будет лежать без движения в земле, а не после убийства, можно считать, что он не мертв, не был ранен, как и все, что осталось от тела.\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.33200531208499334 =========\n",
      ".  Из-за этого, у меня нет возможности ходить на работу. \n",
      ".  Все эти планы можно охарактеризовать так: в начале сентября, как говорится, не было ни одного дня.  Из пяти пунктов и двух (из которых один касается армии) было три.  И, не теряя времени даром, мы начали, как говорится, настраивать себя на то, что будет сделано. \n",
      ".  Я знал, что она все так умеет, и у меня в душе не было сомнений, что в ее взгляде, при воспоминании об этой минутной слабости, она не могла ничего сказать. \n",
      ".  Все это хорошо, но почему-то я не знаю, как быть.  Как понять, что я – девушка?  А, что я – не девушка, что я – пустое место?  Неужели я не понимаю, что значит \"понимаю\", и что значит: \"я – не девушка\"?  Я – не девушка, я\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.398406374501992 =========\n",
      ".  Я думаю, все эти люди не могут быть так, чтобы быть вместе, как и должны быть вместе.  Они не только не были вместе со мною, но как будто были вместе со мной.  Я думаю, что я тоже был частью чего-то другого. \n",
      ".  Влияние на это влияние и привело ее к тому, чтобы начать движение к единству. \n",
      ".  Он чувствовал, что он был занят мыслями, и вдруг почувствовал, что он не в том расположении духа.  Он вспомнил и то, что случилось в этом доме.  Вспомнил, как все смеялись над ним, и вспомнил его слова: \"Какое милое создание!\" — и снова представил себе его улыбку, взгляд, когда он обратился\n",
      ".  А если вы хотите, чтобы в нашем обществе не было ничего, чего нельзя было бы делать, то подумайте, почему бы вам не заняться тем, что есть у вас?  Что вы хотите, чего вы хотите в мире – то, что вы можете сделать? \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.4648074369189907 =========\n",
      ". \n",
      ".  О!  Теперь я понимаю, зачем мне нужен этот новый мир, как только можно его покинуть?  Ты говоришь о новом мире. [Г]\n",
      ".  И, чувствуя себя обманутым и оскорбленным этим, в то же время, был уверен, что положение его будет так же благолепно и легко, как и всегда. \n",
      ".  Они были, очень милые люди, и, наверное, самые счастливые, честные и всесильные люди на свете, которые так нуждаются и рады.  Эти благородные люди хотели бы, чтобы все было отлично, чтобы все было красиво, чтобы все любили и были любимы.  И все это, а также все это для того, чтобы\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5312084993359893 =========\n",
      ".  Здесь надо только заметить, что здесь нет никакой необходимости в том, чтобы как можно скорее, в одну сторону смотреть, как приедем, так и будем.  А приедем мы тогда, потому что все будет готово.  Все будет готово и по законам, и по закону.  А главное, что будет то же самое, что и\n",
      ".  Как только с нею познакомился, ей стало легче, так и помолодела.  К осени, наконец, стало так же.  Муж уезжал в Москву, а дочь, чтобы не ездить к тетке, оставила его с женой и, оставшись одна, опять не вышла на работу. \n",
      ".  Как и другие, эти, к большому удовольствию и прискорбности, были в то же время очень приятны ему, в своем равнодушии, и, наоборот, он находил, что это чувство его было особенно мучительно для самого себя, для других людей.\n",
      ".  Если вы думаете, что с вас требовать, то, как бы вы ни старались, что бы вы ни делали, в этом не будет никакого смысла. \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.5976095617529881 =========\n",
      ".  И мы, все русские, не могли оставаться равнодушными и не можем, чтобы те, которые не могли быть для нас неприятны, мы не можем не чувствовать своей чести и не должны быть должны прощены, когда мы так мало сделали для нее. \n",
      ".  Но даже если вы сейчас поедете в лес, не надо думать, как вам надо ехать.  Во-первых, вы не будете так легко и хорошо говорить, во-вторых, вы не будете так много думать и так скучно говорить, как вы.  Но, во-первых, вы будете не так хорошо думать и не так\n",
      ".  Он, как и всякий человек, очень похож на человека.Он не может ни двигаться, ни бегать, но чувствует в себе человека, который живет его жизнью, и говорит с ним, и чувствует, что он имеет на это право.Признав это существование, Николай II тотчас же уехал в Москву.Слово это он получил только в\n",
      ". \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.6640106241699867 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Configuration saved in ./checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  Но, когда она была здорова и счастлива, и ей не надо было ни думать о чем-то, ни думать, а было очень важно одно – чтобы это было то самое самое счастье, которое она желала себе всю жизнь. \n",
      ".  И, наконец, в 1853 году по возвращении домой в Петербург был вызван по следу на дело.\n",
      ". \n",
      ".  В течение пяти минут при полной темноте и освещенности, на всех углах были расставлены орудия и бомбы, и, наконец, на месте, где они должны были быть, был найден настоящий труп.  Не считая нескольких трупов, в том числе одного мертвого, в этом трупе были только двое русских и три француза.  Из\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.7304116865869854 =========\n",
      ".  И мы с женой пошли на ужин, как вдруг слышим в сенях крик: \n",
      ".  Но не все они были одинаково хороши; как в первом случае, так и во втором — ни одна партия, ни один игрок не был, кроме него самого, очень, кажется, лучшим из тех, которые на первый раз делали это в первые годы своей игры.  Но на этот счет не было другого мнения, кроме одного: очень трудно в\n",
      ".  Это было бы так. \n",
      ".  В начале своего пути (после долгого размышления) он поневольно узнает о том, что и как надо думать.\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.796812749003984 =========\n",
      ".  В начале лета он вернулся в свои родные места, оставив мать и сестру. \n",
      ".  И то, что он хотел сказать, было правдой, но для меня так и осталось тайной, я не знал тогда, что было бы лучше, если бы он сказал, что это хорошо, что он не любит меня, но, видимо, думал, что не понимает, потому что я люблю его.\n",
      ".  В этих словах, с которым и в первый раз в первый раз в его жизни, он был не только поражен словами, но и совершенно серьезно удивлен тому, как точно так же он понимал их и знал, в особенности, если он верил, что им удастся все это сказать.  Он только теперь понял всю глубину значения того, что он\n",
      ".  Да и не только, в таком смысле, в нем много общего, как и в моем прошлом.  Я много думала о том, как, и что нужно делать, и что я для этого сделаю, что бы сделать для того, чтобы быть, как человек, я не только люблю, я люблю, люблю то, что есть и чем\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.8220451527224436 =========\n",
      ".  При этом необходимо учитывать, что у каждого человека бывают свои индивидуальные причины (высокая или маленькая температура воздуха, головная боль, плохое самочувствие). \n",
      ".  С помощью этих советов вы не только избавитесь от боли в желудке и поднимите его аппетит, но и будете знать, как в домашних условиях устранить эту неприятную проблему. \n",
      ". \n",
      ".  Она сидела в глубоком кресле, лицом к лицу с Петром.  По тому, как она старалась подавить страх, он заметил, что она пристально, внимательно следит за каждым его словом.  Она молчала.  Он, в том же положении, в которое его она поставила, знал, что она не захочет говорить ни о чем и потому смотрел на\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to .\n",
      "Configuration saved in ./config.json\n",
      "Configuration saved in ./generation_config.json\n",
      "Model weights saved in ./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_ru_model()\n",
    "freeze_layers(model, 8)\n",
    "finetune(model, tokenizer, TOLST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdOTyRZglnyW",
    "outputId": "50b00223-3c31-4c11-ebe4-53532a4334ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В 2024 году в Петербурге в Петербурге был построен новый типовой дом для петербургского купечества. Князь Дмитрий, не дождавшись разрешения войти в него, вошел с женой в свою спальню и при участии супруги. В ней в комнате в эту минуту был и сын князя Дмитрия, который был в это время один и с матерью и с дядей и, вместе с своею дочерьми, в спальне в комнате княжны. Князь Дмитрий, оставшись в одиночестве в комнате и не смея войти в нее, с своей женой, в это время находился в комнате княжны.\n",
      "В 2024 году Россия пережила первую Великую войну, когда русский флот в виде эскадр дошел до Парижа в 20-х. Русская дипломатия была занята во Франции.\n",
      "В 2024 году, когда она была сделана, ее и причислили к лику святых. После этого в 1812 году она была перевезена в Париж, где была встречена и положена в алтарь, названный в ее честь церковью святого Доминика».\n",
      "В 2024 году, между 2026 и 2026 годами, в составе царского войска был объявлен план об уничтожении русского царя, его семьи и всего русского войска. Для уничтожения царя был применен целый арсенал порохового ружья, в 20 тысяч выстрелов в день при пушечной пальбе; но царь не остановился на том, что все эти выстрелы были совершены в Москве, так как Москва не хотела иметь возможности для нападения на нее. В феврале, за семь дней, после трехмесячной работы, в состав московского войска вошли: московские войска и войска иностранных держав, австрийцы, австрийцы, франкфуртские и прусские войска, а также часть войска Наполеона в Париже. Русские войска были истреблены с большой жестокостью, но не в одно и то же время, и потому их действия не были в состоянии быть произведены в Москве; однако, только после трехмесячной работы в 1825 году Наполеон с русской армией вошел в Россию; через десять дней после того, как Наполеон\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\"В 2024 году\", max_len=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWFyLTz3loHs"
   },
   "source": [
    "# Собрание сочинений Ленина"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "khFg6GmvlpRB",
    "outputId": "19d59697-3cb2-46af-e499-fd8ec865b143"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/f2f7c585b05a16726efe8974586e10b4d5939082/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING N_EPOCHS TO  0.14183550978504744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file /content/gdrive/MyDrive/datasets/cached_lm_GPT2TokenizerFast_256_lenin.txt [took 1.256 s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32598\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 578\n",
      "  Number of trainable parameters = 42528768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='578' max='578' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [578/578 04:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.223600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.012269938650306749 =========\n",
      ".  Он, по сути, борется с \"врагами\" и противополагалниками. (Смех. ).  Он призывает к решительной мере социальной политики к укреплению местного самоуправления.  Это был один из тех уроков, которые не раз были поддернуты.  И сегодня, в условиях мира и мира, мы должны показать,\n",
      ".  В. И. Ленин в своей «Экономической программе», написанной в 1913 году, отмечает: \"Если мы не можем принять такой метод, как ликвидация войны, то нужно отказаться от этого метода во всей его полноте.  В России, в особенности, в области военного строительства и промышленности, не может быть еще достаточного числа таких\n",
      ".  Когда же дотационная комиссия стала рассматривать вопрос о «процессинговой власти», ее «своеобразный» план, а не план, как говорили товарищи из комитета, – «б», – не мог ни в коем случае быть исключен. Теперь-то положение партии в партии окончательно становится таким образом, что нельзя сказать\n",
      ".  В течение 3–4 недель в томатных и поварских цехах (такие как «Запорожсталь» и т. д.) необходимо периодически вводить в растительное и овощеводное отделение, например, по средам в рецептах, например, «Поварские рецепты\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.024539877300613498 =========\n",
      ".  В качестве основы для этого проекта выступает проект \"Социализация молодежи\".  В этой работе мы выступаем не только как руководство, но и как посредник в реализации проекта; это и есть важнейший аспект этой работы, это именно та основная задача, которой должен заниматься, с одной стороны, Маркс, с другой стороны, Энгельс,\n",
      ".  И если бы не эти «знаменитости» и «изящные» их «позы», – если бы не их «позы», – если бы не эти «знаменитости», – если бы не эти «знаменитости» – то в России не осталось бы и следа бы «блестящей\n",
      ".  На эти «добро» должны опираться силы всех, кто не разделяет с ними мнения о положении на этой «добро» (ибо «их» мнение было бы «лучшим»). С другой стороны, если они не согласны с этими «добро», то их не надо бы принуждать или требовать на это согласие, как это\n",
      ".  В качестве примера того, как это работает, можно взять статью В. Г. Таневича «Нравственный контроль» «Обращение» к С. Н. Плеханову, изданную в 1916 году в Петербурге и опубликованную при поддержке издательства «Прогресс». В первой половине XIX века либералы ставили во главу угла вопрос\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.03680981595092025 =========\n",
      ".  Он говорил, что это – в основном политические лозунги: \"Народная революция\", \"демократия\", и все это – на основе сознательных представлений.  И не только сознательных, но и измышленных.  К примеру, не стоит путать большевистскую платформу с марксистско-ленинской, и наоборот, это\n",
      ".  Все три вида рабочих работ могут быть расценены как разовое мероприятие.  Построят в день по несколько сооружений – и то без сноса, не разрушат и не заставят снести несколько десятков домов, не подорвут население. \n",
      ".  Подразумевается также и в том, что мы должны указать в своей резолюции, что у нас есть и крестьянские, и военные и т. д. вопросы, касающиеся не только крестьянской общины, но также и крестьянских общин.  С этой точки зрения очень много вопросов, касающихся сельского хозяйства, можно было бы и обсудить.  Можно\n",
      ".  После чего все были исключены, и не было ни малейшего сомнения, что у этих \"предпринимателей\" были только деньги, которые они брали из кассы или из кармана рабочих.  У нас тоже в руках нет, чтобы мы могли дать указание рабочим, кто будет покупать у них хлеба и давать им хлеб.  Эти рабочие, как\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.049079754601226995 =========\n",
      ".  П. Г. Энгельс, В. И. Ленин, Сочинения.  Т. I, изд. тип. партии, стр. 322–323. \n",
      ".  Все они были \"не на шутку\" заигрывают с представителями рабочих, приучают их к тому, что рабочие являются хозяевами в мире, что рабочие должны всегда стоять на своих обязанностях, заботиться о своем «выполнении\" и выполнении своих обязанностей, что рабочие всегда должны выполнять свой долг перед рабочими и во всяком\n",
      ".  В этом смысле он был весьма похож на тех крестьянских ораторов, которые говорили и о том, как правильно строить партию и как ее применять. \n",
      ".  И только на то время, пока он будет в России, в которой уже имеется не одно такое правительство, мы должны иметь возможность не только в случае нужды, но и в случае крайней необходимости и когда не надо ни по какой иной причине стеснять и не только не позволять, но когда правительство, как нам нужно, в полной мере и не\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.06134969325153374 =========\n",
      ".  Из книги «Философия религии в России» (В.И. Ленин.  Спб., 1909, стр. 7). В этом труде она излагает взгляды Ф. Энгельса на вопрос о причинах революции в Германии. Затем автор доказывает необходимость борьбы пролетариата против «феодализма» и с классами мелкой буржуазии и мелкой\n",
      ".  Во всяком случае, мы видим, что именно из этого источника исходит самая большая доля правды. \n",
      ".  Я и сам считаю себя \"за\" именно то, что в России, так как именно здесь я должен видеть, должны были бы и должны были бы поставить Россию \"на место\" мировой революции.  А то в России \"не только в Германии\" не понимают, что пролетариат в России есть, а в Германии не понимают, что именно\n",
      ".  Все это хорошо, но почему же тогда на словах все и все говорят о том, что они не могут быть уверены в существовании «реальных» проектов? \n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.0736196319018405 =========\n",
      ".  Я говорю о том, что мне было сказано, что на съезде нужно было сказать, чтобы я дал всем, кто не может принять этот ответ, немедленно объявить его резолюцией, это не мое решение. Я не говорю ни слова о том, что это моя резолюция, что мы хотим в нее вступить, что это – не моя резолюция\n",
      ".  В России, в Москве и ряде крупных городов уже более года существуют организованные в Москве группы населения. \n",
      ".  С другой стороны, эти данные и «справедливые» статьи доказывают, что пролетариат в России был на самом деле гораздо сильнее, чем в экономически отсталых странах.\n",
      ".  А если вы хотите, чтобы в нашем обществе не было никакого социального «пролетариата», то вы должны понимать, что вы уже видели, что класс капиталистов был разгромлен. Но, прежде чем вы будете вырезать то, что можно вырезать и зарезать, вы должны будете понять, что они сами себя выбили из борьбы\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.08588957055214724 =========\n",
      ".  И если мы наталкиваемся на какую-то ошибку, то неизбежно находим эту ошибку и наталкиваемся на ошибку с этой стороны, как на не всегда известную, мы говорим, что если бы мы хотели найти ошибку, которая есть наша самая близкая к истине, мы, не имея возможности, а вернее сказать, не имея представления об этом\n",
      ".  О «реальной свободе» и большинстве подобных программ говорится в письме от 25 января, в котором подчеркивается, что \"этот режим больше не соответствует интересам рабочих и что \"все рабочие находятся под их руководством\", а \"вся масса рабочих должна работать в единственном числе\" и т. д.\n",
      ".  И, наконец, именно на этом основании мы будем требовать в проекте проекта указания места и состава рабочей группы, которая будет проводить исследование.  Поэтому в проекте нет указания места рабочей группы и состава рабочей группы, которая будет проводить исследование.  Вместо этого мы должны сказать, что такой категорический разнобой между членами рабочей группы и тем, что они сами\n",
      ".  Они были противниками того, чтобы вся семья была в лагере. В декабре 1912 года они вступили в партию с.-д. – и были избраны в Думу. С.-д. вошли в социал-демократическую фракцию. В 1912 году их фракция осталась во втором числе (в 1913–1914 годах они еще состояли в\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.09815950920245399 =========\n",
      ".  Эти два направления одинаково хорошо характеризуют себя во всем буржуазном обществе и в России в особенности.  Если не удастся полностью овладеть искусством диалектики, то только для этого необходим и этот метод, потому что именно он – диалектика вообще.  Этот метод особенно необходим для социализации народа. \n",
      ".  Как только с нами в одной точке окажутся все, так у нас будет уверенность, что мы не подведем до последнего.  Не подведем до конца.  Мы сделаем все, чтобы победить эту группу, как можно скорее.  Мы должны знать, что мы в силах и на что надо иметь время, чтобы победить это отчаянное отставание\n",
      ".  С этой точки зрения деятельность партии определяется в основном, как борьба за единство партии с массой, как борьба за диктатуру рабочих, за полную свободу и диктатуру крестьян, а в то же время, – как борьба за право каждого крестьянина, для которого право выбора и обязанность выбора являются делом семьи и государства, – что «в этой\n",
      ".  Если они в этом преуспевают, значит, и им принадлежат, и принадлежат.  Но если они преуспевают с точки зрения интересов своей деятельности, это значит, что они находятся на вершине социальной лестницы.  Не может быть и речи ни о каком «социальном развитии» в качестве общественного существа, а лишь об общественном состоянии, потому что это\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.11042944785276074 =========\n",
      ".  И мы, все русские, не отрицаем того, что наша власть, несмотря на всю свою враждебность к этому движению, все-таки продолжает вести с ним борьбу.  Мы видим, что большинство наших противников, несмотря на все усилия нашей политики, продолжают в том же русле продолжать борьбу против народа.  И они будут идти по тому\n",
      ".  Это – самая сильная революция русского национального движения в первые годы. \n",
      ".  На днях, в своем первом номере \"Пролетарского Рабочего», редакция встретила заметку, в которой в качестве предисловия к статье приводятся слова профессора Московского университета, профессора Киевского университета, И. К. Иоффе о событиях, происходящих в Украине.\n",
      ".  \"Социал-Демократ\" No 1 (286) ноября 1910 г. в газете \"Социал-Демократ\" № 1 (286) ноября 1910 года напечатано в редакции журнала \"Сопротивления» № 207. В. И. Ленин. «Социал-Демократ\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.12269938650306748 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Configuration saved in ./checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  Но, когда они были созданы в единстве и единстве, они были только средством борьбы против «грязного империализма». Итак, марксизм представляет из себя одну из форм господства буржуазии над пролетариатом, ибо капитализм – есть империалист, а пролетариат – в то же время эксплуатирует капиталистические массы\n",
      ".  И, наконец, в 1853 г. по инициативе социал-демократа И. В. Мартова (М. Е. – в «Письмах Н. Л.) он был избран в Думу. – 164. – 168, 228–230.\n",
      ".  О том, как на самом деле должны бороться за Россию социал-демократы, с одной стороны, и большинство социал-демократов, с другой, – мы говорим в том, что мы знаем, что это происходит при участии передовых социал-демократов (впоследствии марксистов и меньшевиков), которые, к сожалению, большеви\n",
      ".  В \"Брюсселяре\" был принят ряд воззваний к пролетариату и к крестьянству, в частности, о необходимости скорейшего сближения его с капитализмом, о его борьбе против буржуазии, против социализма и пр. В. Ленин писал в своей статье «Воля к освобождению крестьянства» (1920)\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.13496932515337423 =========\n",
      ".  И мы с женой тоже на них ссылаемся, а потому и сами на них ссылаемся.  А теперь мы с женой на нее ссылаемся, а мы – на нее.  И, если у нас нет никакого намерения, то пусть все, кто на нас ссылается, не обижаются.  Так почему же они должны обижаться\n",
      ".  Но не все они могут быть правы; как не всегда можно и не всегда нужно иметь дело с правыми, правыми или правыми, – вот в чем, кажется, суть их взаимоотношений. И вот как обстоит дело с правомерностью того или другого? Что является правомерным в правом поведении этих политических партий? Это вопрос в\n",
      ".  Это доказывает тот факт, что не только в России и во Франции, и в Германии, и в Италии, и во Франции, что нет ни одного государства, которое не было бы уничтожено и уничтожено по частям или по частям.  Не может быть ни одного государства, где нет таких крупных капиталистических предприятий, где капиталистическая революция не\n",
      ".  В начале 1906 года Ленин писал письмо, в котором писал: \"Поражает ли, когда мы ставим вопросы, которые ставит перед собой Россия, когда на нее обрушивается весь груз капиталистического угнетения и вся тяжесть ее борьбы\", и указывал, что \"эта война уже принесла свои результаты, но и тогда борьба будет еще более жестокой и\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== progress: 0.14184049079754601 =========\n",
      ".  Однако для нас, всех этих групп, мы должны стремиться к тому, чтобы не только в виде исключения, но и в виде исключения из нашей политической партии, в партии, которая бы представляла собой более сплоченную и сплоченную группу.Наоборот, это может идти вразрез с политической необходимостью, может быть, с экономической потребностью, или же\n",
      ".  В сущности, это даже и в голову не приходило, что на деле, хотя бы и теоретически, большевики являются демократами, которые ведут борьбу с революцией.  К. Маркс, например, доказывал, что только так можно провести социалистическую революцию, а затем и победу революции. \n",
      ".  За свою свободу он и в революцию не стал, но дело его, как говорят, в том, что он не стал говорить об этом, а говорил о политике. \n",
      ".  А в самом деле, что такого в нашей статье в пользу революционного правительства?  На что я не должен и никогда не должен смотреть, чтобы мы, большевики, думали, что дело партии пойдет на пользу социалистам.  Это только смешно.  Это значит: мы будем молчать, когда будет такая позиция социал-демократической рабочей партии!\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to .\n",
      "Configuration saved in ./config.json\n",
      "Configuration saved in ./generation_config.json\n",
      "Model weights saved in ./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_ru_model()\n",
    "freeze_layers(model, 6)\n",
    "finetune(model, tokenizer, LENIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbIYI5M5Yud-",
    "outputId": "495d3d4a-5edc-45ef-8ba6-448789add3b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". самое важное, что мы можем делать, это создавать что бы то ни было более или менее в форме, когда-либо, в форме движения, или иначе, в форме организации.\n",
      ". самое важное, чего хочет достигнуть буржуазный народ в этом столетии, – это перехода к демократии. И именно поэтому – борьба с буржуазной демократией, как бы она ни была вредна для буржуазии в России, как бы она ни была вредна для всех угнетенных наций в мире. Но именно благодаря этому переходу в России может теперь быть и буржуазная демократия. Но в этой демократии не должны быть неучтены и те условия, о которых говорит буржуазия. – В России нет ни одного сколько-нибудь известного демократического принципа, по которому нельзя было бы строить демократию. И здесь – только одна гарантия, что буржуазия в России не должна будет превращаться в какую-нибудь одну политическую партию. Но при всем разграничении с буржуазией в России можно добиться и этого. Борьба с буржуазной демократией – это – только первая ступень. Вторая ступень – это – борьба с оппортунизмом в России. Это – борьба с оппортунизмом не только\n",
      ". самое важное дело: не обманывай себя, если ты не хочешь быть обманщиком!\n",
      ". самое важное, что он должен принять на себя все тяготы и опасности.\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\". самое важное\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwYUzmsHYxQc",
    "outputId": "9821a909-18ce-4408-e054-b6f2c6d1a2aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". я верю, что мы, – сказал Фокс, не отрываясь от окна. – Мы должны быть готовы защищать себя сами. Мы не хотим ни о какой защите ни просить, ни просить, а должны бороться со всеми этими противниками. Но только мы не хотим, чтоб эти противники были из-за угла, из-за забора, с улицы, в деревне, в своем городе, чтобы они не могли жить в собственном доме, чтобы не могли ни в какой мере внять нашему пожеланию.\n",
      ". я верю, что в этом вопросе есть смысл. Но, пожалуй, не было бы в этой работе ни одной правильной, но чрезвычайно полезной, работы, которая бы в корне изменила всю жизнь и которую нужно проводить со всем старанием и любовью. «Отчего бы это» — скажете вы, а именно: «отчего бы это»–то, вы совершенно правы! Но мы опять же не можем с этим согласиться, ибо «отчего бы это»–то, мы еще увидим, но совершенно не в этом роде, а в следующем. В этой работе мы видим, что она только на счет «снижения» (отождествления и т. д. – вместе с тем вы увидите, что речь идет о «несуществе», а не об «снижении») и этот факт совершенно меняет дело. В этой работе мы видим, что мы не можем прямо сказать, что это именно то, чем является «всякая всячина»,\n",
      ". я верю, что это было со времен моей молодости, в то время как в судьбу нашей революции мы живем при капитализме и в настоящем капитализме, в одной стране, и мы живем при тех же условиях в другой стране, которые были и будем существовать до тех пор, пока наша революция не примет в свои руки нового революционного порядка. Революция теперь уже становится сама собой в нашей стране и не только в России, но и на других, более отдаленных областях, в которых эта революция есть. Она уже начинается. И, следовательно, наша революция, как бы глубоко и упорно ни были в наших руках, всегда будет идти дальше, вперед, и в то время, когда наша страна начинает подъем, революционное движение в ней еще не состоялось. Она будет идти до конца, но она должна стать той революционной революцией, которая заставит и Россию, и Россию объединиться.\n",
      ". я верю, что в России найдется трудовая партия с ее сплоченной и твердой организацией. И для этого должна быть мощная и крепкая рабочая партия, сплоченная сплоченной, надежной и широкой партией. Но не у нас, а у всех рабочих классов, ибо не у нас должна быть партия, если она не сплочена, не сплочена в одну единую, сплоченную и монолитную организацию.\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\". я верю, что\", max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp2I5W7MZGiv",
    "outputId": "3dda70e9-e063-4b64-f3d6-f752fa30c377"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". я ненавижу либералов-декабристов-монархов и их «патриотические» «указатели» и отдельные слова-лирики. Я не ненавижу либералов-монофизитов, а в том, что они называют либералов «монархами» и т. и м. в слово (отдельно) «монархи», я вижу не в том, что они думают, а в том, что они видят и говорят (вместе с теми либералами, которые говорят: «монофизиками», «пролетариями», «дворцами» и т. д.). Но они не говорят, а говорят, что они говорят и делают (и на самом деле они говорят о том, что и на самом деле говорят о себе), говорят, что они делают, говорят, что они все делают правильно, ибо они могут только только говорить, но не делать не могут ничего другого, кроме\n",
      ". я ненавижу их, и я презираю их. Не знаю, сколько времени прошло со времени нашей встречи, но я уже знаю, что они не могут оставаться такими, как до сих пор. Я понимаю, что они не могут больше жить без нас. Я говорю им, что они ничего не поняли, что они будут вечно в рабстве, что они будут терпеть, пока им дадут в долг. Я вижу, что они все, вместе с ними, все их друзья будут страдать. Я понимаю, что они скажут: если им дадут взаймы или, по крайней мере, позволят разойтись, если они пойдут войной, то только к их смерти. Мы все должны сделать то же. Мы должны положить конец этой бойне, чтобы дать нам возможность спокойно жить, все так, как нам положено по закону. Но мы не должны и не должны забывать об этом. Нельзя допустить того, чтобы эти условия были не соблюдены, иначе мы не сможем продолжать борьбу. И только при таких условиях,\n",
      ". я ненавижу и его, но все-таки, мне кажется, мы должны найти с его стороны какие-нибудь объяснения. Но с тем же успехом, в каком я убежден, они могут помочь нам с этой точки зрения. С точки зрения философии, я думаю, их и моих учителей нельзя обвинить в разжигании раздора, а они сами сказали, что они занимаются политической работой, что они думают и пишут, что они пишут о необходимости свергнуть правительство.\n",
      ". я ненавижу это, но то, что я говорю, это не что иное, как выражение того, что такое и для чего я это говорю. – Вот как? – Вот именно, что это не что иное, как выражение того, что такое и для чего мы это говорим. Вот так. А разве это не понятно?\n"
     ]
    }
   ],
   "source": [
    "print_generations(prefix=\". я ненавижу\", max_len=200)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d844a15082c45c6aaf3b937a058657d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16fc3fcd6e7a45008c032a6714bb8fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a12a378d8584f0eaa8871094e393e02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b4ed3ca990d486fb294ab96854492ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b50fb01b9504b3fa62c165ea09efb8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d2164e5963741a19a75a17fc4cf189a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d7e6d4452c64f44971cfc90ce746316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c8a59195d847bc858d3dfbb3098975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df0c5d4f42484bc9bd45e9f0f7e25f06",
      "placeholder": "​",
      "style": "IPY_MODEL_35b7cfeaeb4440b195ac62660daee437",
      "value": " 1.71M/1.71M [00:00&lt;00:00, 3.34MB/s]"
     }
    },
    "2165a701e63f46d79d95a2f23370288f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3964cbce98614d28a753b3ea6d75ae19",
       "IPY_MODEL_30f06e4dd9534bd18ec055ad0f8b13ed",
       "IPY_MODEL_708fd078cdd3481e984567d4b3184fb2"
      ],
      "layout": "IPY_MODEL_4daf0a2782194095870c2f8e2b58aa4e"
     }
    },
    "283c2885475e42aaa2a91d6d6774698e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82862eaf05a0463e85d57748672b4137",
      "max": 608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8da6771f27484ef99f721e8f2516a1c0",
      "value": 608
     }
    },
    "300bb98f8b7a413ea48c59ecb8fcae15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30f06e4dd9534bd18ec055ad0f8b13ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f918b06459eb41f198fa3d25acb7e432",
      "max": 1270925,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_300bb98f8b7a413ea48c59ecb8fcae15",
      "value": 1270925
     }
    },
    "35b7cfeaeb4440b195ac62660daee437": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3964cbce98614d28a753b3ea6d75ae19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f285b73d6b6043f6b9da603623864ad7",
      "placeholder": "​",
      "style": "IPY_MODEL_0d844a15082c45c6aaf3b937a058657d",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "3a76393c12444a40a15e0e92424318d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1e6a8d6b6ed4f80b0fe6bf8f1fc8ba6",
      "max": 551290714,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9dc44856f3f6476082f165be4d5633e4",
      "value": 551290714
     }
    },
    "3b862b1b4070492a915298d1820fa477": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4279e77945f142e7be40a4fcdf4df5dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4daf0a2782194095870c2f8e2b58aa4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63d424f5ad7b4c7bbd6159ab1bd86a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b862b1b4070492a915298d1820fa477",
      "max": 1713123,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8212133bd0b148d39c3a99772308c52a",
      "value": 1713123
     }
    },
    "6d2fe73c34b848f1a92faaf491a4dbf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87b10810f84a44c9952f573af6ed169e",
      "placeholder": "​",
      "style": "IPY_MODEL_1a12a378d8584f0eaa8871094e393e02",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "708fd078cdd3481e984567d4b3184fb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d7e6d4452c64f44971cfc90ce746316",
      "placeholder": "​",
      "style": "IPY_MODEL_4279e77945f142e7be40a4fcdf4df5dc",
      "value": " 1.27M/1.27M [00:00&lt;00:00, 2.95MB/s]"
     }
    },
    "77072bba96f245b49e7b6f2359dfe18b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8212133bd0b148d39c3a99772308c52a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "82862eaf05a0463e85d57748672b4137": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83b8479702394ba0ae62d762cf5d3879": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87b10810f84a44c9952f573af6ed169e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d4b2911c90846a295fe9beda5e5b131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6e3fab9b28e4035a87728317b4f9745",
       "IPY_MODEL_3a76393c12444a40a15e0e92424318d6",
       "IPY_MODEL_f0ab9bc2bd0a4106b009a7843d416c6d"
      ],
      "layout": "IPY_MODEL_1d2164e5963741a19a75a17fc4cf189a"
     }
    },
    "8da6771f27484ef99f721e8f2516a1c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8df61434b9ee4ce28bdea6506dfc7577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b77909843924d7c8bfb7193aaa02e35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dc44856f3f6476082f165be4d5633e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b842b1ad95f8445e9e7bc12664e2437d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d13ee81975ed4923a5f6afdb104dfa51",
      "placeholder": "​",
      "style": "IPY_MODEL_16fc3fcd6e7a45008c032a6714bb8fea",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c1e6a8d6b6ed4f80b0fe6bf8f1fc8ba6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d13ee81975ed4923a5f6afdb104dfa51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd1e731d438c421cb55140abe5653a38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d2fe73c34b848f1a92faaf491a4dbf0",
       "IPY_MODEL_63d424f5ad7b4c7bbd6159ab1bd86a64",
       "IPY_MODEL_20c8a59195d847bc858d3dfbb3098975"
      ],
      "layout": "IPY_MODEL_83b8479702394ba0ae62d762cf5d3879"
     }
    },
    "df0c5d4f42484bc9bd45e9f0f7e25f06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df3e9558a2c442ec99d352fc7a94e655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b842b1ad95f8445e9e7bc12664e2437d",
       "IPY_MODEL_283c2885475e42aaa2a91d6d6774698e",
       "IPY_MODEL_eb2196d7d3d34aafa010cc89ac28d67c"
      ],
      "layout": "IPY_MODEL_77072bba96f245b49e7b6f2359dfe18b"
     }
    },
    "eb2196d7d3d34aafa010cc89ac28d67c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b50fb01b9504b3fa62c165ea09efb8d",
      "placeholder": "​",
      "style": "IPY_MODEL_f8ad791396824a99a5cb32ee4ed2abe0",
      "value": " 608/608 [00:00&lt;00:00, 5.54kB/s]"
     }
    },
    "f0ab9bc2bd0a4106b009a7843d416c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b4ed3ca990d486fb294ab96854492ea",
      "placeholder": "​",
      "style": "IPY_MODEL_8df61434b9ee4ce28bdea6506dfc7577",
      "value": " 551M/551M [00:04&lt;00:00, 166MB/s]"
     }
    },
    "f285b73d6b6043f6b9da603623864ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37cb7a17ed14e5db0894004de143887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6e3fab9b28e4035a87728317b4f9745": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b77909843924d7c8bfb7193aaa02e35",
      "placeholder": "​",
      "style": "IPY_MODEL_f37cb7a17ed14e5db0894004de143887",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f8ad791396824a99a5cb32ee4ed2abe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f918b06459eb41f198fa3d25acb7e432": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
